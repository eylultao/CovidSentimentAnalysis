round(Question7(Beta, X, "cauchit"),3)
round(Question7(Beta, X, "cloglog"),3)
round(Question7(Beta, X, "cauchit"),3)
Question7=function(Beta, X, link)
{
n_holdout = dim(X)[1]
preds = c()
for (i in (1:n_holdout)){
rowi = Beta*X[i,]
sumi = sum(rowi)
preds = c(preds, sumi)
}
print(preds)
if(link == "logit"){
return(1/(1+exp(-preds)))
}
if(link == "probit"){
return(pnorm(preds))
}
if(link == "cloglog"){
return(1 - exp(-exp(preds)))
}
if(link == "cauchit"){
return(1/pi *atan(preds) + 1/2)
}
}
round(Question7(Beta, X, "probit"),3)
round(Question7(Beta, X, "logit"),3)
round(Question7(Beta, X, "cauchit"),3)
round(Question7(Beta, X, "cloglog"),3)
knitr::opts_chunk$set(echo = TRUE)
data(LakeHuron)
LakeHuron.train <- window(LakeHuron, start = 1875, end = 1967)
LakeHuron.test <- window(LakeHuron, start = 1968, end = 1972)
plot(LakeHuron.train)
data(LakeHuron)
LakeHuron.train <- window(LakeHuron, start = 1875, end = 1967) #first 93
LakeHuron.test <- window(LakeHuron, start = 1968, end = 1972) # last 5
plot(LakeHuron.train)
acf(LakeHuron.train)
pacf(LakeHuron.train)
AR.model <- arima(LakeHuron.train, order = c(2,0,0))
AR.model
tsdiag(AR.model)
forecast <- forecast(AR.model, h = 3, level = 0.95)
forecast <- forecast(AR.model, h = 3, level = 0.95)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(zoo)
library(tseries)
forecast <- forecast(AR.model, h = 3, level = 0.95)
forecast <- forecast(AR.model, h = 3, level = 0.95)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(zoo)
library(tseries)
data(LakeHuron)
LakeHuron.train <- window(LakeHuron, start = 1875, end = 1967) #first 93
LakeHuron.test <- window(LakeHuron, start = 1968, end = 1972) # last 5
plot(LakeHuron.train)
acf(LakeHuron.train)
pacf(LakeHuron.train)
AR.model <- arima(LakeHuron.train, order = c(2,0,0))
AR.model
tsdiag(AR.model)
forecast <- forecast(AR.model, h = 3, level = 0.95)
forecast <- predict(AR.model, h = 3, level = 0.95)
forecast
forecast <- predict(AR.model, h = 3, prediction.interval = TRUE, level = 0.95)
forecast
forecast <- predict(AR.model, h = 3, prediction.interval = TRUE, level = 0.95)
forecast
forecast <- predict(AR.model,n.ahead = 3, prediction.interval = TRUE, level = 0.95)
forecast
forecast <- predict(AR.model,n.ahead = 3, prediction.interval = TRUE, level = 0.95)
print(forecast)
forecast <- predict(AR.model,n.ahead = 3, prediction.interval = TRUE, level = 0.95)
print(forecast)
knitr::opts_chunk$set(echo = TRUE)
#if you do not have the package, type install.packages("name_of_the_package")
library(knitr)
library(zoo)
library(tseries)
forecast1 <- predict(hw_model, n.ahead = 12, prediction.interval = TRUE, level = 0.95)
forecast3 <- predict(AR.model,n.ahead = 3, prediction.interval = TRUE, level = 0.95)
print(forecast3)
LakeHuron.test[1:3]
LakeHuron.test[1:3]
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(zoo)
library(tseries)
library(forecast)
library(forecastSNSTS)
# MSPE is just the mean of the prediction error squared, so we can just calculate it without a function
MSPE_HW = mean((test_set - holtw_forecast53_95) ^ 2)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(zoo)
library(tseries)
library(forecast)
library(forecastSNSTS)
data <- read.csv("vandata-edit-wk.csv")
mean_temp <- data$Mean
mean_temp_ts <- ts(mean_temp, start = 2013, end = c(2021,0), frequency = 53)
train_set <- window(mean_temp_ts, start = 2013, end = c(2020, 0))
test_set <- window(mean_temp_ts, start = 2020)
plot(train_set, xlab = "Year", ylab = "Mean Temperature in celcius",
main = "Weekly Mean Temperature in celcius")
s = 53
lagmax = s*5
pacf(train_set, lag.max =lagmax)
acf(train_set, lag.max =lagmax)
sarima010 <- arima(train_set, order = c(0,0,0), seasonal = list(order = c(0,1,0)))
plot(sarima010$residuals)
pacf(sarima010$residuals, lag.max = lagmax)
acf(sarima010$residuals, lag.max = lagmax)
sarima011 <- arima(train_set, order = c(0,0,0), seasonal = list(order = c(0,1,1)))
sarima011
plot(sarima011$residuals)
acf(sarima011$residuals, lag.max = lagmax)
pacf(sarima011$residuals, lag.max = lagmax)
sarima100011 <- arima(train_set, order = c(1,0,0), seasonal = list(order = c(0,1,1)) )
sarima100011
boxj_forecast53 <- predict(sarima100011, n.ahead=53) #next 53, PI not specified
boxj_forecast53_95 <- forecast(sarima100011, h=53, level =0.95) #next 53, PI level = 95%, used forecast to get interval
boxj_forecast53_95
plot(test_set, ylab = "Values", main= "Comparison of True value and 95% prediction interval")
lines(boxj_forecast53$pred, col = "blue")
lines(boxj_forecast53_95$lower, col = "red")
lines(boxj_forecast53_95$upper, col = "green")
legend("topleft", legend = c("True values","Point Predictions",
"Lower Prediction Interval","Upper Predicted Interval"),
col = c("black", "blue", "red", "green"), lty=1)
holtw <- HoltWinters(train_set)
#holtw
holtw_forecast53_95 <- predict(holtw, n.ahead = 53, prediction.interval = TRUE, level = 0.95)
#holtw_forecast53_95
holtw_forecast_ts <- ts(holtw_forecast53_95[,1], start = 2020, end = c(2021,0), frequency = 53)
#holtw_forecast_ts
holtw_forecast_lowr_ts <- ts(holtw_forecast53_95[,3], start = 2020, end = c(2021,0), frequency = 53)
holtw_forecast_uppr_ts <- ts(holtw_forecast53_95[,2], start = 2020, end = c(2021,0), frequency = 53)
plot(test_set, main="Comparison of True values with respect to Box-Jenkins and HoltWinters predictions")
lines(boxj_forecast53$pred, col = "blue")
lines(boxj_forecast53_95$lower, col = "red")
lines(boxj_forecast53_95$upper, col = "red")
lines(holtw_forecast_ts, col = "green")
lines(holtw_forecast_lowr_ts, col = "yellow")
lines(holtw_forecast_uppr_ts, col = "yellow")
legend("topleft", legend = c("True values",
"Box-Jenkins point predictions",
"Box-Jenkins Lower Prediction Interval",
"Box-Jenkins Upper Prediction Interval",
"Holt-Winters point predictions",
"Holt-Winters Lower Prediction Interval",
"Holt-Winters Upper Prediction Interval"),
col = c("black", "blue", "red", "red", "green", "yellow", "yellow"), lty=1)
# MSPE is just the mean of the prediction error squared, so we can just calculate it without a function
MSPE_HW = mean((test_set - holtw_forecast53_95) ^ 2)
# MSPE is just the mean of the prediction error squared, so we can just calculate it without a function
MSPE_HW = mean((test_set - holtw_forecast53_95) ^ 2)
MSPE_BJ = mean((test_set - boxj_forecast53) ^ 2)
# MSPE is just the mean of the prediction error squared, so we can just calculate it without a function
MSPE_HW = mean((test_set - holtw_forecast53_95) ^ 2)
MSPE_BJ = mean((test_set - boxj_forecast53$pred) ^ 2)
data2 <- read.csv("vandata-edit-mo.csv")
mean_temp2 <- data2$Mean
mean_temp_ts <- ts(mean_temp_mo, start = 2013, end = c(2020,12), frequency = 12)
data2 <- read.csv("vandata-edit-mo.csv")
mean_temp2 <- data2$Mean
mean_temp_ts <- ts(mean_temp2, start = 2013, end = c(2020,12), frequency = 12)
train_set2 <- window(mean_temp_ts, start = 2013, end = c(2020, 0))
test_set2 <- window(mean_temp_ts, start = 2020)
plot(train_set2, xlab = "Year", ylab = "Mean Temperature in Celcius",
main = "Mean Monthly Temperatures in Celcius")
data2 <- read.csv("vandata-edit-mo.csv")
mean_temp2 <- data2$Mean
mean_temp_ts <- ts(mean_temp2, start = 2013, end = c(2020,12), frequency = 12)
train_set2 <- window(mean_temp_ts, start = 2013, end = c(2020, 0))
test_set2 <- window(mean_temp_ts, start = 2020)
plot(train_set2, xlab = "Year", ylab = "Mean Temperature in Celcius",
main = "Mean Monthly Temperatures in Celcius")
s2 = 12 # monthly
lagmax2 = s2*5
acf(train_set2, lag.max = lagmax2)
pacf(train_set2, lag.max = lagmax2)
sarima010_2 <- arima(train_set2, order = c(0,0,0), seasonal = list(order=c(0,1,0)))
sarima010_2
sarima010_2 <- arima(train_set2, order = c(0,0,0), seasonal = list(order=c(0,1,0)))
sarima010_2
plot(sarima010_2$residuals)
acf(sarima010_2$residuals, lag.max = 7*s2)
pacf(sarima010_2$residuals, lag.max = 7*s2)
plot(sarima010_2$residuals)
acf(sarima010_2$residuals, lag.max = 5*s2)
pacf(sarima010_2$residuals, lag.max = 5*s2)
plot(sarima1$residuals, main="Attempt 1 with SARIMA(000)x(010)")
sarima1 <- arima(train_set2, order = c(0,0,0), seasonal = list(order=c(0,1,0)))
sarima1
plot(sarima1$residuals, main="Attempt 1 with SARIMA(000)x(010)")
acf(sarima1$residuals, lag.max = 5*s2)
pacf(sarima1$residuals, lag.max = 5*s2)
sarima2 <- arima(train_set2, order = c(0,0,0), seasonal = list(order=c(0,1,1))) # Q=1, D= 1, P = 0
sarima2
plot(sarima2$residuals, main="Second Attempt with SARIMA(000)x(011)")
acf(sarima2$residuals, lag.max = 5*s2)
pacf(sarima2$residuals, lag.max = 5*s2)
sarima3 <- arima(train_set2, order = c(1,0,1), seasonal = list(order=c(0,1,1))) # Q=1, D= 1, P = 0, d=0, q=1, p=1
sarima3
plot(sarima3$residuals, main="Second Attempt with SARIMA(000)x(011)")
acf(sarima3$residuals, lag.max = 5*s2)
pacf(sarima3$residuals, lag.max = 5*s2)
plot(sarima3$residuals, main="Third Attempt with SARIMA(101)x(011)")
acf(sarima3$residuals, lag.max = 5*s2)
pacf(sarima3$residuals, lag.max = 5*s2)
plot(sarima3$residuals, main="Third Attempt with SARIMA(101)x(011)")
acf(sarima3$residuals, lag.max = 5*s2)
pacf(sarima3$residuals, lag.max = 5*s2)
tsdiag(sarima3)
alpha = 0.5
beta = 0.2
ans.a <- (1 + beta^2 + 2 * alpha * beta) / (1 - alpha^2)
ans.a <- round(ans.a, 4)
ans.b <- (alpha + beta + alpha * beta^2 + alpha^2 * beta) / (1 - alpha^2)
ans.b <- round(ans.b, 4)
ans.c <- alpha * ans.b
ans.c <- round(ans.c, 4)
ans.d <- (1 + 2 * beta * cos(omega) + beta^2) / (pi * (1 + alpha^2 - 2 * alpha * cos(omega)) )
ans.d <- round(ans.d, 4)
alpha = 0.5
beta = 0.2
omega = 0.628
ans.a <- (1 + beta^2 + 2 * alpha * beta) / (1 - alpha^2)
ans.a <- round(ans.a, 4)
ans.b <- (alpha + beta + alpha * beta^2 + alpha^2 * beta) / (1 - alpha^2)
ans.b <- round(ans.b, 4)
ans.c <- alpha * ans.b
ans.c <- round(ans.c, 4)
ans.d <- (1 + 2 * beta * cos(omega) + beta^2) / (pi * (1 + alpha^2 - 2 * alpha * cos(omega)) )
ans.d <- round(ans.d, 4)
ans.a
ans.b
ans.c
ans.d
ans.a <- pi^2 / 2
ans.a <- round(ans.a, 4)
ans.a
n=3
if( n %% 2 == 0 ){
ans.b = 0
ans.c = 2/ n^2
}else{
ans.b =  2/ n^2
ans.c = 0
}
beta1 = 0.76
omega1 = 0.449
sigma = 2.46
temp_a = 1 / pi * (1 + 2 * beta1 * cos(omega1) / (1 + beta1^2))
ans.a <- (1 + beta1^2) * sigma^2 * temp_a
ans.a <- round(ans.a, 4)
ans.a
beta2 = 0.15
omega2 = 0.393
ans.b <- sigma^2 / pi * (2 + beta2^2 + 2 * ( (1 + beta2)*cos(omega2) + beta2 * cos(2*omega2)  ))
ans.b <- round(ans.b, 4)
ans.b
beta3 = 0.59
beta4 = 0.11
omega3 = 0.524
ans.c <- sigma^2 / pi * ( 1 + beta3^2 + beta4^2 + 2 * ( (beta3 - beta3*beta4)*cos(omega3) - beta4* cos(2*omega3) ) )
ans.c <- round(ans.c, 4)
ans.c
training_set <- df[indices, ]
setwd("/Users/eylulaygun/Desktop/Year\ 5/stat\ 447B/CovidSentimentAnalysis")
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06.csv")
# new column, response variable: popularity score
df$popularity_score <- 0.75*(df$retweets) + 0.25*(df$favorites)
print(summary(df$popularity_score))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 10, 1000), labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
print(table(df$popularity_score_ctg))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 10, 1000))#, labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
print(table(df$popularity_score_ctg))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000))#, labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 8, 1000))#, labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
print(table(df$popularity_score_ctg))
print(table(df$popularity_score_ctg))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000)), labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
print(table(df$popularity_score_ctg))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000), labels = c("LOW", "AVG", "HIGH"))
plot(df$popularity_score_ctg)
print(table(df$popularity_score_ctg))
print(head(df[df$popularity_score == "HIGH"]))
print(head(df[df$popularity_score == "HIGH", ]))
print(head(df[df$popularity_score_ctg == "HIGH", ]))
print(head(df[df$popularity_score_ctg == "HIGH", ]))
hist(df$popularity_score)
hist(df$popularity_score, c(0, 1,2,3,4,5,6,7,8,9,10,15,20,25,40,60,80,100,200,400,600,800))
hist(df$popularity_score, c(0, 1,2,3,4,5,6,7,8,9,10,15,20,25,40,60,80,100,200,400,600,1000))
hist(df$popularity_score, c(0, 1,2,3,4,5,6,7,8,9,10,15,20,25,40,60,80,100,200,400,600,900))
hist(df$popularity_score < 500, c(0, 1,2,3,4,5,6,7,8,9,10,15,20,25,40,60,80,100,200,400,600,900))
hist(df$popularity_score)
getwd()
plot(df$popularity_score)
plot(density(df$popularity_score))
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2000)
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2000)
df <- df[-low_popularity_sample]
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06.csv")
# new column, response variable: popularity score
df$popularity_score <- 0.75*(df$retweets) + 0.25*(df$favorites)
print(summary(df$popularity_score))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000), labels = c("LOW", "AVG", "HIGH"))
# new columns, explanatory variables:
df$date <- as.POSIXct(df$date,tz=Sys.timezone(), format ="%Y-%m-%d") # remove H-M-Sec from date
# new column, turn compound score to ordinal categorical
df$overall_sentiment <- cut(df$compound_score, breaks = c(-1, -0.05, 0.05, 1), labels =  c("NEG", "NEU", "POS"))
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2000)
df <- df[-low_popularity_sample,]
plot(df$popularity_score_ctg)
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06.csv")
# new column, response variable: popularity score
df$popularity_score <- 0.75*(df$retweets) + 0.25*(df$favorites)
print(summary(df$popularity_score))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000), labels = c("LOW", "AVG", "HIGH"))
# new columns, explanatory variables:
df$date <- as.POSIXct(df$date,tz=Sys.timezone(), format ="%Y-%m-%d") # remove H-M-Sec from date
# new column, turn compound score to ordinal categorical
df$overall_sentiment <- cut(df$compound_score, breaks = c(-1, -0.05, 0.05, 1), labels =  c("NEG", "NEU", "POS"))
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2500)
df <- df[-low_popularity_sample,]
plot(df$popularity_score_ctg)
setwd("/Users/eylulaygun/Desktop/Year\ 5/stat\ 447B/CovidSentimentAnalysis")
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06.csv")
# new column, response variable: popularity score
df$popularity_score <- 0.75*(df$retweets) + 0.25*(df$favorites)
print(summary(df$popularity_score))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000), labels = c("LOW", "AVG", "HIGH"))
# new columns, explanatory variables:
df$date <- as.POSIXct(df$date,tz=Sys.timezone(), format ="%Y-%m-%d") # remove H-M-Sec from date
# new column, turn compound score to ordinal categorical
df$overall_sentiment <- cut(df$compound_score, breaks = c(-1, -0.05, 0.05, 1), labels =  c("NEG", "NEU", "POS"))
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2500)
df <- df[-low_popularity_sample,] # dropping half of low popularity data
plot(df$popularity_score_ctg) # checking new distribution
# Split data into training and testing sets
n <- dim(df)[1]
training_set_size = 3000 # initial size, will increase later due to moving duplicate users
testing_set_size = n - training_set_size
set.seed(420)
indices <- sample(0:n, size = training_set_size, replace = FALSE)
training_set <- df[indices, ]
testing_set <- df[-indices,]
# ensure no user has tweets in both training set and testing set
duplicate_user_indices = which(testing_set$user_name %in% training_set$user_name)
tweets_to_move = testing_set[duplicate_user_indices,]
testing_set = testing_set[-duplicate_user_indices,]
training_set = rbind(training_set, tweets_to_move)
# ensure no more duplicates
which(testing_set$user_name %in% training_set$user_name) # should output 0
setwd("/Users/eylulaygun/Desktop/Year\ 5/stat\ 447B/CovidSentimentAnalysis")
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06.csv")
# new column, response variable: popularity score
df$popularity_score <- 0.75*(df$retweets) + 0.25*(df$favorites)
print(summary(df$popularity_score))
df$popularity_score_ctg <- cut(df$popularity_score, breaks = c(-0.1, 1, 9.9, 1000), labels = c("LOW", "AVG", "HIGH"))
# new columns, explanatory variables:
df$date <- as.POSIXct(df$date,tz=Sys.timezone(), format ="%Y-%m-%d") # remove H-M-Sec from date
# new column, turn compound score to ordinal categorical
df$overall_sentiment <- cut(df$compound_score, breaks = c(-1, -0.05, 0.05, 1), labels =  c("NEG", "NEU", "POS"))
# undersampling LOW popularity as they make majority of data
plot(df$popularity_score_ctg)
low_popularity_indices <- which(df$popularity_score_ctg == "LOW")
low_popularity_sample <- sample(low_popularity_indices, size = 2500)
df <- df[-low_popularity_sample,] # dropping half of low popularity data
plot(df$popularity_score_ctg) # checking new distribution
# Split data into training and testing sets
n <- dim(df)[1]
training_set_size = 2500 # initial size, will increase later due to moving duplicate users
testing_set_size = n - training_set_size
set.seed(420)
indices <- sample(0:n, size = training_set_size, replace = FALSE)
training_set <- df[indices, ]
testing_set <- df[-indices,]
# ensure no user has tweets in both training set and testing set
duplicate_user_indices = which(testing_set$user_name %in% training_set$user_name)
tweets_to_move = testing_set[duplicate_user_indices,]
testing_set = testing_set[-duplicate_user_indices,]
training_set = rbind(training_set, tweets_to_move)
# ensure no more duplicates
which(testing_set$user_name %in% training_set$user_name) # should output 0
# save new csv
write.csv(df, "updated_dataframe-apr06-2.csv")
write.csv(testing_set, "testing_set.csv")
write.csv(training_set, "training_set.csv")
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06-2.csv")
training_set <- read.csv("training_set.csv")
testing_set <- read.csv("testing_set.csv")
# Data visualizations
summary(df)
plot(table(df$overall_sentiment))
hist(df$compound_score, c(-1, -0.5, -0.1, 0.1, 0.5, 1))  # distribution, skewed towards positive
hist(df$compound_score, c(-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1))  # distribution, skewed towards positive
hist(df$compound_score,  c(-1, -0.9, -0.8,-0.7, -0.6, -0.5,-0.4,-0.3, -0.2,-0.1, 0,0.1, 0.2,0.3, 0.4, 0.5, 0.6, 0.7, 0.8,0.9, 1))  # distribution, skewed towards positive
sum(df$compound_score> 0.05)
sum(df$compound_score< -0.05)
df$retweets_ctg <- cut(df$retweets, breaks = c(-0.01, 0.5, 1, 5, 1000), labels =  c("NO", "AVG", "YES", "VYES"))
df$favorites_ctg <- cut(df$favorites, breaks = c(-0.01, 0.5, 1, 5, 1000), labels =  c("NO", "AVG", "YES", "VYES"))
print(table(df$retweets_ctg))
print(table(df$favorites_ctg))
print(df$retweets>2 )
summary(df$retweets)
summary(df$favorites)
cor.test(df$retweets, df$favorites,  method = "spearman") # reject null hypothesis
cor.test(df$retweets, df$compound_score, method = "spearman") # can't reject
cor.test(df$favorites, df$compound_score, method = "spearman") # reject null hypothesis
labels =  c("VNEG", "NEG", "NEU", "POS", "VPOS")
par(mfrow=c(2,3))
for (i in 1:5){
sentiment_i_retweets = df$retweets2[which(df$overall_sentiment == labels[i])]
sentiment_i_scores = df$compound_score[which(df$overall_sentiment == labels[i])]
print(head(sentiment_i_scores))
print(labels[i])
#boxplot(sentiment_i_scores, xlab = labels[i], ylab= "scores")
boxplot(sentiment_i_retweets, xlab = labels[i], ylab= "retweets^(1/4)")
}
table(training_set$user_location)
table(training_set$user_verified)
table(testing_set$user_verified)
par(mfrow=c(2,2))
# see how average compound score has changed over time for tweets
mean_score_by_day = aggregate(training_set$compound_score, by = list(training_set$date), FUN = mean )
plot( x = as.factor(mean_score_by_day$Group.1), y= mean_score_by_day$x, ylab = "Mean Score")
max_score_by_date =  aggregate(training_set$compound_score, by = list(training_set$date), FUN = max )
plot( x = as.factor(max_score_by_date$Group.1), y= max_score_by_date$x, ylab = "Max Score")
min_score_by_date = aggregate(training_set$compound_score, by= list(training_set$date), FUN = min)
plot( x = as.factor(min_score_by_date$Group.1), y= min_score_by_date$x, ylab = "Min Score") # increasing trend
median_score_by_date = aggregate(training_set$compound_score, by= list(training_set$date), FUN = median)
plot( x = as.factor(median_score_by_date$Group.1), y= median_score_by_date$x, ylab = "Median Score") # slightly? increase, most values are zero.
difference_min_max = max_score_by_date$x - min_score_by_date$x # over time, tweets get "milder"
par(mfrow=c(2,3))
# How does number of hashtags change in terms of tweet sentiment
for (i in 1:5){
sentiment_i_hashtag_count = df$hashtags_count[which(df$overall_sentiment == labels[i])]
sentiment_i_scores = df$compound_score[which(df$overall_sentiment == labels[i])]
print(head(sentiment_i_hashtag_count))
print(labels[i])
#boxplot(sentiment_i_scores, xlab = labels[i], ylab= "scores")
boxplot(sentiment_i_hashtag_count, xlab = labels[i], ylab= "hashtag count")
}
getwd()
setwd("/Users/eylulaygun/Desktop/Year\ 5/stat\ 447B/CovidSentimentAnalysis")
# read dataframe and perform basic transformations
df <- read.csv("updated_dataframe-apr06-2.csv", stringsAsFactors = T)
training_set <- read.csv("training_set.csv", stringsAsFactors = T)
testing_set <- read.csv("testing_set.csv", stringsAsFactors = T)
# Create new variable: popularity score
# this will be our response variable for the rest of project
# Intuitively, growth in reach generated by retweets is not linear but exponential (just like spreading covid!),
# and with slight increments in retweets we can still achieve large increases in the reach of a tweet, which is what makes it popular
# eg. avg twitter user has ~200 followers.
#     If avg user tweets something, for each retweet they get,
#     their tweet will be visible in ~200 other users feed. (assuming no overlap between followers)
#training_set$popularity_score = (0.75*(training_set$retweets^(1/4)) + 0.25*(training_set$favorites^(1/4)))
# new column, turn compound score to ordinal categorical
#training_set$popularity_score_ctg <- cut(training_set$popularity_score, breaks = c(-0.01, 0.2, 1, 1.75, 690), labels =  c("NO", "AVG", "YES", "VYES"))
plot(training_set$popularity_score, training_set$compound_score)
summary(training_set$popularity_score)
plot(table(training_set$popularity_score_ctg))
training_set$retweets_inv <- 1/ (training_set$retweets  +1 )
training_set$retweets_ctg <- cut(training_set$retweets, breaks = c(-0.01, 0.5, 1, 5, 1000), labels =  c("NO", "AVG", "YES", "VYES"))
table(training_set$retweets_ctg)
plot(table(training_set$retweets_ctg))
summary(training_set$retweets)
summary(training_set$favorites)
logit1= glm(popularity_score_ctg ~ hashtags_count + compound_score + overall_sentiment,family=binomial(link="logit"),data=training_set)
summary(logit1)
model2 <- rpart(popularity_score_ctg ~ hashtags_count + compound_score , data=training_set)
library(rpart)
library(rpart.plot)
model2 <- rpart(popularity_score_ctg ~ hashtags_count + compound_score , data=training_set)
par(mfrow=c(1,1))
rpart.plot(model2, box.palette = "blue")
tree_out_pred =predict(model2,newdata=testing_set)
boxplot(training_set$hashtags_count, training_set$retweets_ctg)
